{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86cb1bf5",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dba756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "import ast\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd817fd2",
   "metadata": {},
   "source": [
    "# Variables for global usage and finalized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50d3bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_consequents, all_antecedents, all_conf, all_lift = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bca73e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_rg(dataset, min_support_ = 0.01):\n",
    "    antecedents, consequents, consequent_conf, consequent_lift = [], [], [], []\n",
    "    transactions = []\n",
    "    for x in dataset:\n",
    "        transactions.append(x)\n",
    "    rules = list(apriori(transactions, min_support = min_support_, min_confidence = 0.7, min_lift = 1.5))\n",
    "    \n",
    "    print(f\"*** Total number of rules formulated: {len(rules)} ***\")\n",
    "    \n",
    "    for i in rules:\n",
    "        base_item_set = str(i[2][0].items_base).removeprefix(\"frozenset({\").removesuffix(\"})\")\n",
    "        add_item_set = str(i[2][0].items_add).removeprefix(\"frozenset({\").removesuffix(\"})\")\n",
    "        confidence = f\"{round(i[2][0].confidence * 100,2)}\"\n",
    "        lift = f\"{round(i[2][0].lift,2)}\"\n",
    "        if add_item_set not in consequents:\n",
    "            antecedents.append(base_item_set)\n",
    "            consequents.append(add_item_set)\n",
    "            consequent_conf.append(float(confidence))\n",
    "            consequent_lift.append(float(lift))\n",
    "            if add_item_set not in all_consequents:\n",
    "                all_antecedents.append(base_item_set)\n",
    "                all_consequents.append(add_item_set)\n",
    "                all_conf.append(float(confidence))\n",
    "                all_lift.append(float(lift))\n",
    "        else:\n",
    "            prev_conf = consequent_conf[consequents.index(add_item_set)]\n",
    "            if(float(confidence) > prev_conf):\n",
    "                antecedents[consequents.index(add_item_set)] = base_item_set\n",
    "                all_antecedents[consequents.index(add_item_set)] = base_item_set\n",
    "                consequent_conf[consequents.index(add_item_set)] = float(confidence)\n",
    "                all_conf[consequents.index(add_item_set)] = float(confidence)\n",
    "                consequent_lift[consequents.index(add_item_set)] = float(lift)\n",
    "                all_lift[consequents.index(add_item_set)] = float(lift)\n",
    "    print(f\"*** Total number of strong rules: {len(consequents)} ***\")\n",
    "    print(\"Market Basket Analysis Rules\", end=\"\\n\\n\")\n",
    "    for i in range(len(consequents)):\n",
    "        print(f\"Item set 1: {antecedents[i]}\\nItem set 2: {consequents[i]}\\nconfidence = {consequent_conf[i]}%\\nlift = {consequent_lift[i]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03ff6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower the support value, higher will be the number of rules and unique rules generated.\n",
      "the higher the support value, lesser will be the rules generated but will be more quality rules.\n",
      "Press enter to go with default settings.\n",
      "\n",
      "Enter number of customer segments for rule generation: 3\n",
      "Enter dataset path: elite_transactions\n",
      "\n",
      "Enter desired support value (default 1%): 2\n",
      "\n",
      "Rules will be based on support of 2.0%\n",
      "*** Total number of rules formulated: 43 ***\n",
      "*** Total number of strong rules: 3 ***\n",
      "Market Basket Analysis Rules\n",
      "\n",
      "Item set 1: 'BISCUITS', 'COLD & FROZEN FOODS', 'BAKERY ITEMS', 'FLOOR CLEANERS'\n",
      "Item set 2: 'DAIRY'\n",
      "confidence = 79.66%\n",
      "lift = 1.65\n",
      "\n",
      "Item set 1: 'DETERGENTS', 'DISPOSABLES/ PARTY SUPPLIES'\n",
      "Item set 2: 'FLOOR CLEANERS'\n",
      "confidence = 72.92%\n",
      "lift = 1.68\n",
      "\n",
      "Item set 1: 'TEA', 'NAMKEENS'\n",
      "Item set 2: 'BISCUITS'\n",
      "confidence = 73.91%\n",
      "lift = 2.92\n",
      "\n",
      "Enter dataset path: freq_transactions\n",
      "\n",
      "Enter desired support value (default 1%): 2\n",
      "\n",
      "Rules will be based on support of 2.0%\n",
      "*** Total number of rules formulated: 668 ***\n",
      "*** Total number of strong rules: 16 ***\n",
      "Market Basket Analysis Rules\n",
      "\n",
      "Item set 1: 'DETERGENTS', 'COFFEE'\n",
      "Item set 2: 'BAKERY ITEMS'\n",
      "confidence = 95.0%\n",
      "lift = 2.34\n",
      "\n",
      "Item set 1: 'BISCUITS', 'BAKERY ITEMS', 'CHIPS', 'NAMKEENS'\n",
      "Item set 2: 'DAIRY'\n",
      "confidence = 90.91%\n",
      "lift = 2.31\n",
      "\n",
      "Item set 1: 'HEALTHDRINKS', 'CHOCOLATES'\n",
      "Item set 2: 'BISCUITS'\n",
      "confidence = 100.0%\n",
      "lift = 2.94\n",
      "\n",
      "Item set 1: 'DAIRY', 'BATH SOAP & BODYWASH', 'INSTANT SNACKS'\n",
      "Item set 2: 'FLOOR CLEANERS'\n",
      "confidence = 100.0%\n",
      "lift = 2.06\n",
      "\n",
      "Item set 1: 'BISCUITS', 'TEA', 'DAL&PULSES'\n",
      "Item set 2: 'FLOURS'\n",
      "confidence = 82.61%\n",
      "lift = 4.33\n",
      "\n",
      "Item set 1: 'WHOLE SPICES', 'WHOLE CEREALS'\n",
      "Item set 2: 'DAL&PULSES'\n",
      "confidence = 84.0%\n",
      "lift = 5.69\n",
      "\n",
      "Item set 1: 'COFFEE', 'JUICES'\n",
      "Item set 2: 'CARBONATED'\n",
      "confidence = 70.0%\n",
      "lift = 3.67\n",
      "\n",
      "Item set 1: 'BAKERY ITEMS', 'COFFEE', 'JUICES'\n",
      "Item set 2: 'COLD & FROZEN FOODS'\n",
      "confidence = 76.0%\n",
      "lift = 3.5\n",
      "\n",
      "Item set 1: 'BATH SOAP & BODYWASH', 'BAKERY ITEMS', 'INSTANT SNACKS'\n",
      "Item set 2: 'BISCUITS', 'FLOOR CLEANERS'\n",
      "confidence = 90.48%\n",
      "lift = 4.33\n",
      "\n",
      "Item set 1: 'DETERGENTS', 'INSTANT SNACKS'\n",
      "Item set 2: 'BISCUITS', 'BAKERY ITEMS'\n",
      "confidence = 70.27%\n",
      "lift = 3.71\n",
      "\n",
      "Item set 1: 'CARBONATED', 'BAKERY ITEMS', 'COFFEE'\n",
      "Item set 2: 'JUICES'\n",
      "confidence = 70.37%\n",
      "lift = 7.64\n",
      "\n",
      "Item set 1: 'DETERGENTS', 'BAKERY ITEMS', 'FLOOR CLEANERS'\n",
      "Item set 2: 'INSTANT SNACKS'\n",
      "confidence = 75.76%\n",
      "lift = 3.43\n",
      "\n",
      "Item set 1: 'BISCUITS', 'DAIRY', 'MASALA & GROUND SPICE  POWDER'\n",
      "Item set 2: 'NAMKEENS'\n",
      "confidence = 74.07%\n",
      "lift = 3.7\n",
      "\n",
      "Item set 1: 'HEALTHDRINKS', 'CHOCOLATES'\n",
      "Item set 2: 'BISCUITS', 'DAIRY'\n",
      "confidence = 82.61%\n",
      "lift = 5.25\n",
      "\n",
      "Item set 1: 'BISCUITS', 'MASALA & GROUND SPICE  POWDER', 'DAL&PULSES'\n",
      "Item set 2: 'WHOLE SPICES'\n",
      "confidence = 71.43%\n",
      "lift = 6.41\n",
      "\n",
      "Item set 1: 'BISCUITS', 'BATH SOAP & BODYWASH', 'DAL&PULSES'\n",
      "Item set 2: 'FLOURS', 'FLOOR CLEANERS'\n",
      "confidence = 76.92%\n",
      "lift = 7.26\n",
      "\n",
      "Enter dataset path: cotcon_transactions\n",
      "\n",
      "Enter desired support value (default 1%): 2\n",
      "\n",
      "Rules will be based on support of 2.0%\n",
      "*** Total number of rules formulated: 18 ***\n",
      "*** Total number of strong rules: 4 ***\n",
      "Market Basket Analysis Rules\n",
      "\n",
      "Item set 1: 'DAIRY', 'BREAKFAST CEREALS'\n",
      "Item set 2: 'FLOOR CLEANERS'\n",
      "confidence = 81.58%\n",
      "lift = 1.9\n",
      "\n",
      "Item set 1: 'BISCUITS', 'COLD & FROZEN FOODS'\n",
      "Item set 2: 'BAKERY ITEMS'\n",
      "confidence = 76.09%\n",
      "lift = 2.5\n",
      "\n",
      "Item set 1: 'SUGAR', 'BAKERY ITEMS'\n",
      "Item set 2: 'DAIRY'\n",
      "confidence = 77.78%\n",
      "lift = 1.85\n",
      "\n",
      "Item set 1: 'DAL&PULSES', 'UPWAS MAIN ITEMS'\n",
      "Item set 2: 'FLOURS'\n",
      "confidence = 70.45%\n",
      "lift = 5.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"The lower the support value, higher will be the number of rules and unique rules generated.\n",
    "the higher the support value, lesser will be the rules generated but will be more quality rules.\n",
    "Press enter to go with default settings.\"\"\")\n",
    "loop = int(input(\"\\nEnter number of customer segments for rule generation: \"))\n",
    "for i in range(loop):\n",
    "    try:\n",
    "        path = input(\"Enter dataset path: \")\n",
    "        user_support = float(input(\"\\nEnter desired support value (default 1%): \"))/100\n",
    "        print(f\"\\nRules will be based on support of {user_support*100}%\")\n",
    "        apriori_rg(pd.read_csv(f'./{path}.csv', header = None)[0][1:].apply(ast.literal_eval), user_support)\n",
    "    except ValueError:\n",
    "        print(\"\\nRules will be based on support of 1%\")\n",
    "        apriori_rg(pd.read_csv(f'./{path}.csv', header = None)[0][1:].apply(ast.literal_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4f034",
   "metadata": {},
   "source": [
    "# Model Artifact Name convention genertation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a912f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_now = str(datetime.datetime.now())\n",
    "list_dt = dt_now.split()\n",
    "cur_date, curr_time =  list_dt[0], list_dt[1][:5].replace(':', '-')\n",
    "\n",
    "name_conv = f\"{cur_date}--{curr_time}_MBA_rules\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e3292",
   "metadata": {},
   "source": [
    "# CSV conversion (OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57c5f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_conv = []\n",
    "for i in range(len(all_consequents)):\n",
    "    antecedent = all_antecedents[i]\n",
    "    consequent = all_consequents[i]\n",
    "    confidence_ = all_conf[i]\n",
    "    lift_ = all_lift[i]\n",
    "    tran = [antecedent, consequent, confidence_, lift_]\n",
    "    csv_conv.append(tran)\n",
    "df = pd.DataFrame(csv_conv, columns=['Antecedent', 'Consequent', 'Confidence','Lift'])\n",
    "name_conv\n",
    "df.to_csv(f'{name_conv}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
